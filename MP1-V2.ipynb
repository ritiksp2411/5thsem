{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MP1",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN603HfKGSfYugqdbvJz5na",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritiksp2411/5thsem/blob/master/MP1-V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeKw243WiksF"
      },
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import time\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from keras.layers.pooling import GlobalMaxPooling2D, GlobalAveragePooling2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.layers import Dense, Dropout, Conv2D, Flatten, Activation, BatchNormalization, Add\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "import os.path\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v8c3Kv4i0gj"
      },
      "source": [
        "class VIPPruning():\n",
        "    __name__ = 'VIP Pruning'\n",
        "\n",
        "    def __init__(self, n_comp=2, model=None, layers=[], representation='max', percentage_discard=0.1, face_verif=False):\n",
        "        if len(layers) == 0:\n",
        "            self.layers = list(range(1, len(model.layers)))#Starts by one since the 0 index is the Input\n",
        "        else:\n",
        "            self.layers = layers\n",
        "\n",
        "        if representation == 'max':\n",
        "            self.pool = GlobalMaxPooling2D()\n",
        "        elif representation == 'avg':\n",
        "            self.pool = GlobalAveragePooling2D()\n",
        "        else:\n",
        "            self.pool = representation\n",
        "\n",
        "        self.n_comp = n_comp\n",
        "        self.scores = None\n",
        "        self.score_layer = None\n",
        "        self.idx_score_layer = []\n",
        "        self.template_model = model\n",
        "        self.conv_net = self.custom_model(model=model, layers=self.layers)\n",
        "        self.percentage_discard = percentage_discard\n",
        "        self.face_verif = face_verif\n",
        "\n",
        "    def custom_model(self, model, layers):\n",
        "        input_shape = model.input_shape\n",
        "        input_shape = (input_shape[1], input_shape[2], input_shape[3])\n",
        "        inp = Input(input_shape)\n",
        "\n",
        "        feature_maps = [Model(model.input, self.pool(model.get_layer(index=i).output))(inp) for i in layers if isinstance(model.get_layer(index=i), Conv2D)]\n",
        "        # feature_maps = []\n",
        "        # for i in layers:\n",
        "        #     layer = model.get_layer(index=i)\n",
        "        #     if isinstance(layer, Conv2D):\n",
        "        #         H = Model(model.input, self.pool(layer.output))\n",
        "        #         H = H(inp)\n",
        "        #         feature_maps.append(H)\n",
        "\n",
        "        self.layers = list(range(0, len(feature_maps)))\n",
        "        model = Model(inp, feature_maps)\n",
        "        return model\n",
        "\n",
        "    def flatten(self, features):\n",
        "        n_samples = features[0].shape[0]\n",
        "        X = None\n",
        "        for layer_idx in range(0, len(self.layers)):\n",
        "            if X is None:\n",
        "                X = features[layer_idx].reshape((n_samples,-1))\n",
        "                self.idx_score_layer.append((0, X.shape[1]-1))\n",
        "            else:\n",
        "                X_tmp = features[layer_idx].reshape((n_samples,-1))\n",
        "                self.idx_score_layer.append((X.shape[1], X.shape[1]+X_tmp.shape[1] - 1))\n",
        "                X = np.column_stack((X, X_tmp))\n",
        "\n",
        "        X = np.array(X)\n",
        "        return X\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if self.face_verif == True:\n",
        "            faces1 = self.conv_net.predict(X[:, 0, :])\n",
        "            faces2 = self.conv_net.predict(X[:, 1, :])\n",
        "            faces1 = self.flatten(faces1)\n",
        "            faces2 = self.flatten(faces2)\n",
        "            X = np.abs(faces1 - faces2)#Make lambda function\n",
        "        else:\n",
        "            X = self.conv_net.predict(X)\n",
        "            X = self.flatten(X)\n",
        "\n",
        "        pls_model = PLSRegression(n_components=self.n_comp, scale=True)\n",
        "        pls_model.fit(X, y)\n",
        "        self.scores = self.vip(X, y, pls_model)\n",
        "        self.score_by_filter()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def vip(self, x, y, model):\n",
        "        # Adapted from https://github.com/scikit-learn/scikit-learn/issues/7050\n",
        "        t = model.x_scores_\n",
        "        w = model.x_weights_\n",
        "        q = model.y_loadings_\n",
        "\n",
        "        m, p = x.shape\n",
        "        _, h = t.shape\n",
        "\n",
        "        vips = np.zeros((p,))\n",
        "\n",
        "        # s = np.diag(t.T @ t @ q.T @ q).reshape(h, -1)\n",
        "        s = np.diag(np.dot(np.dot(np.dot(t.T, t), q.T), q)).reshape(h, -1)\n",
        "        total_s = np.sum(s)\n",
        "\n",
        "        for i in range(p):\n",
        "            weight = np.array([(w[i, j] / np.linalg.norm(w[:, j])) ** 2 for j in range(h)])\n",
        "            #vips[i] = np.sqrt(p * (s.T @ weight) / total_s)\n",
        "            vips[i] = np.sqrt(p * (np.dot(s.T, weight)) / total_s)\n",
        "\n",
        "        return vips\n",
        "\n",
        "    def find_closer_th(self, percentage=0.1, allowed_layers=[]):\n",
        "        scores = None\n",
        "        for i in range(0, len(self.score_layer)):\n",
        "            if i in allowed_layers:\n",
        "                if scores is None:\n",
        "                    scores = self.score_layer[i]\n",
        "                else:\n",
        "                    scores = np.concatenate((scores, self.score_layer[i]))\n",
        "\n",
        "        total = scores.shape[0]\n",
        "        closest = np.zeros(total)\n",
        "        for i in range(0, total):\n",
        "            th = scores[i]\n",
        "            idxs = np.where(scores <= th)[0]\n",
        "            discarded = len(idxs) / total\n",
        "            closest[i] = abs(percentage - discarded)\n",
        "\n",
        "        th = scores[np.argmin(closest)]\n",
        "        return th\n",
        "\n",
        "    def score_by_filter(self):\n",
        "        model = self.template_model\n",
        "        self.score_layer = []\n",
        "        idx_Conv2D = 0\n",
        "\n",
        "        for layer_idx in range(1, len(model.layers)):\n",
        "\n",
        "            layer = model.get_layer(index=layer_idx)\n",
        "\n",
        "            if isinstance(layer, Conv2D):\n",
        "                weights = layer.get_weights()\n",
        "\n",
        "                n_filters = weights[0].shape[3]\n",
        "\n",
        "                begin, end = self.idx_score_layer[idx_Conv2D]\n",
        "                score_layer = self.scores[begin:end + 1]\n",
        "                features_filter = int((len(self.scores[begin:end]) + 1) / n_filters)\n",
        "\n",
        "                score_filters = np.zeros((n_filters))\n",
        "                for filter_idx in range(0, n_filters):\n",
        "                    score_filters[filter_idx] = np.mean(score_layer[filter_idx:filter_idx + features_filter])\n",
        "\n",
        "                self.score_layer.append(score_filters)\n",
        "                idx_Conv2D = idx_Conv2D + 1\n",
        "\n",
        "        return self\n",
        "\n",
        "    def idxs_to_prune(self,  X_train=None, y_train=None, allowed_layers=[]):\n",
        "        output = []\n",
        "\n",
        "        self.fit(X_train, y_train)\n",
        "\n",
        "        # If 0 means that all layers are allow to pruning\n",
        "        if len(allowed_layers) == 0:\n",
        "            allowed_layers = list(range(0, len(self.template_model.layers)))\n",
        "\n",
        "        th = self.find_closer_th(percentage=self.percentage_discard, allowed_layers=allowed_layers)\n",
        "\n",
        "        model = self.template_model\n",
        "        idx_Conv2D = 0\n",
        "        for layer_idx in range(0, len(model.layers)):\n",
        "\n",
        "            layer = model.get_layer(index=layer_idx)\n",
        "\n",
        "            if isinstance(layer, Conv2D):\n",
        "\n",
        "                if idx_Conv2D in allowed_layers:\n",
        "                    score_filters = self.score_layer[idx_Conv2D]\n",
        "\n",
        "                    idxs = np.where(score_filters <= th)[0]\n",
        "                    if len(idxs) == len(score_filters):\n",
        "                        print('Warning: All filters at layer [{}] were selected to be removed'.format(layer_idx))\n",
        "                        idxs = []\n",
        "\n",
        "                    output.append((layer_idx, idxs))\n",
        "\n",
        "                idx_Conv2D = idx_Conv2D + 1\n",
        "\n",
        "        return output"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuP69cLcjEhG",
        "outputId": "b5880a86-6c61-447f-e7b6-aa4454638499"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.classification import accuracy_score\n",
        "import argparse\n",
        "import keras\n",
        "from keras.layers.pooling import GlobalMaxPooling2D, GlobalAveragePooling2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.layers import Dense, Dropout, Conv2D, Flatten, Activation, BatchNormalization, Add\n",
        "from keras.layers import Input\n",
        "from keras.models import Model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk2vhGl9jHrU"
      },
      "source": [
        "def layers_to_prune(model):\n",
        "    # Convert index into Conv2D index (required by pruning methods)\n",
        "    idx_Conv2D = 0\n",
        "    output = []\n",
        "    for i in range(0, len(model.layers)):\n",
        "        if isinstance(model.get_layer(index=i), Conv2D):\n",
        "            output.append(idx_Conv2D)\n",
        "            idx_Conv2D = idx_Conv2D + 1\n",
        "\n",
        "    #Exception for VGG-Based architectures\n",
        "    output.pop(-1)\n",
        "    return output\n",
        "\n",
        "def rebuild_net(model=None, layer_filters=[]):\n",
        "    n_discarded_filters = 0\n",
        "    total_filters = 0\n",
        "    model = model\n",
        "    inp = (model.inputs[0].shape.dims[1].value,\n",
        "           model.inputs[0].shape.dims[2].value,\n",
        "           model.inputs[0].shape.dims[3].value)\n",
        "\n",
        "    H = Input(inp)\n",
        "    inp = H\n",
        "    idxs = []\n",
        "    idx_previous = []\n",
        "\n",
        "    for i in range(0, len(model.layers)+1):\n",
        "\n",
        "        try:\n",
        "            layer = model.get_layer(index=i)\n",
        "        except:\n",
        "            break\n",
        "        config = layer.get_config()\n",
        "\n",
        "        if isinstance(layer, MaxPooling2D):\n",
        "            H = MaxPooling2D.from_config(config)(H)\n",
        "\n",
        "        if isinstance(layer, Dropout):\n",
        "            H = Dropout.from_config(config)(H)\n",
        "\n",
        "        if isinstance(layer, Activation):\n",
        "            H = Activation.from_config(config)(H)\n",
        "\n",
        "        if isinstance(layer, BatchNormalization):\n",
        "            weights = layer.get_weights()\n",
        "            weights[0] = np.delete(weights[0], idx_previous)\n",
        "            weights[1] = np.delete(weights[1], idx_previous)\n",
        "            weights[2] = np.delete(weights[2], idx_previous)\n",
        "            weights[3] = np.delete(weights[3], idx_previous)\n",
        "            H = BatchNormalization(weights=weights)(H)\n",
        "\n",
        "        elif isinstance(layer, Conv2D):\n",
        "            weights = layer.get_weights()\n",
        "\n",
        "            n_filters = weights[0].shape[3]\n",
        "            total_filters = total_filters + n_filters\n",
        "\n",
        "            #idxs = [item for item in layer_filters if item[0] == i][0][1]\n",
        "            idxs = [item for item in layer_filters if item[0] == i]\n",
        "            if len(idxs)!=0:\n",
        "                idxs = idxs[0][1]\n",
        "\n",
        "            weights[0] = np.delete(weights[0], idxs, axis=3)\n",
        "            weights[1] = np.delete(weights[1], idxs)\n",
        "            n_discarded_filters += len(idxs)\n",
        "            if len(idx_previous) != 0:\n",
        "                weights[0] = np.delete(weights[0], idx_previous, axis=2)\n",
        "\n",
        "            config['filters'] = weights[1].shape[0]\n",
        "            H = Conv2D(activation=config['activation'],\n",
        "                       activity_regularizer=config['activity_regularizer'],\n",
        "                       bias_constraint=config['bias_constraint'],\n",
        "                       bias_regularizer=config['bias_regularizer'],\n",
        "                       data_format=config['data_format'],\n",
        "                       dilation_rate=config['dilation_rate'],\n",
        "                       filters=config['filters'],\n",
        "                       kernel_constraint=config['kernel_constraint'],\n",
        "                       # config=config['config'],\n",
        "                       # scale=config['scale'],\n",
        "                       kernel_regularizer=config['kernel_regularizer'],\n",
        "                       kernel_size=config['kernel_size'],\n",
        "                       name=config['name'],\n",
        "                       padding=config['padding'],\n",
        "                       strides=config['strides'],\n",
        "                       trainable=config['trainable'],\n",
        "                       use_bias=config['use_bias'],\n",
        "                       weights=weights\n",
        "                       )(H)\n",
        "\n",
        "        elif isinstance(layer, Flatten):\n",
        "            H = Flatten()(H)\n",
        "\n",
        "        elif isinstance(layer, Dense):\n",
        "            weights = layer.get_weights()\n",
        "            weights[0] = np.delete(weights[0], idx_previous, axis=0)\n",
        "            H = Dense(units=config['units'],\n",
        "                      activation=config['activation'],\n",
        "                      activity_regularizer=config['activity_regularizer'],\n",
        "                      bias_constraint=config['bias_constraint'],\n",
        "                      bias_regularizer=config['bias_regularizer'],\n",
        "                      kernel_constraint=config['kernel_constraint'],\n",
        "                      kernel_regularizer=config['kernel_regularizer'],\n",
        "                      name=config['name'],\n",
        "                      trainable=config['trainable'],\n",
        "                      use_bias=config['use_bias'],\n",
        "                      weights=weights)(H)\n",
        "            idxs = []#After the first Dense Layer the methods stop prunining\n",
        "\n",
        "        idx_previous = idxs\n",
        "    #print('Percentage of discarded filters {}'.format(n_discarded_filters / float(total_filters)))\n",
        "    return Model(inp, H)\n",
        "\n",
        "def count_filters(model):\n",
        "    n_filters = 0\n",
        "    for layer_idx in range(1, len(model.layers)):\n",
        "\n",
        "        layer = model.get_layer(index=layer_idx)\n",
        "        if isinstance(layer, keras.layers.Conv2D) == True:\n",
        "            config = layer.get_config()\n",
        "            n_filters+=config['filters']\n",
        "\n",
        "    return n_filters\n",
        "\n",
        "def compute_flops(model):\n",
        "    import keras\n",
        "    from keras.models import Model\n",
        "    from keras.layers import Input,Conv2D\n",
        "    from tensorflow.keras.layers import DepthwiseConv2D\n",
        "    total_flops =0\n",
        "    flops_per_layer = []\n",
        "\n",
        "    for layer_idx in range(1, len(model.layers)):\n",
        "        layer = model.get_layer(index=layer_idx)\n",
        "        if isinstance(layer, DepthwiseConv2D) is True:\n",
        "            _, output_map_H, output_map_W, current_layer_depth = layer.output_shape\n",
        "\n",
        "            _, _, _, previous_layer_depth = layer.input_shape\n",
        "            kernel_H, kernel_W = layer.kernel_size\n",
        "\n",
        "            #Computed according to https://arxiv.org/pdf/1704.04861.pdf Eq.(5)\n",
        "            flops = (kernel_H * kernel_W * previous_layer_depth * output_map_H * output_map_W) + (previous_layer_depth * current_layer_depth * output_map_W * output_map_H)\n",
        "            total_flops += flops\n",
        "            flops_per_layer.append(flops)\n",
        "\n",
        "        elif isinstance(layer, keras.layers.Conv2D) is True:\n",
        "            _, output_map_H, output_map_W, current_layer_depth = layer.output_shape\n",
        "\n",
        "            _, _, _, previous_layer_depth = layer.input_shape\n",
        "            kernel_H, kernel_W = layer.kernel_size\n",
        "\n",
        "            flops = output_map_H * output_map_W * previous_layer_depth * current_layer_depth * kernel_H * kernel_W\n",
        "            total_flops += flops\n",
        "            flops_per_layer.append(flops)\n",
        "\n",
        "        if isinstance(layer, keras.layers.Dense) is True:\n",
        "            _, current_layer_depth = layer.output_shape\n",
        "\n",
        "            _, previous_layer_depth = layer.input_shape\n",
        "\n",
        "            flops = current_layer_depth * previous_layer_depth\n",
        "            total_flops += flops\n",
        "            flops_per_layer.append(flops)\n",
        "\n",
        "    return total_flops, flops_per_layer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RinhvzUSkwE6"
      },
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras import regularizers\n",
        "from tensorflow.keras import optimizers"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgGqzWMxjcmU",
        "outputId": "09d91173-3cd8-46b2-d398-0f47b03a6ac9"
      },
      "source": [
        "np.random.seed(12227)\n",
        "iterations = 5\n",
        "p = 0.05\n",
        "epochs = 10\n",
        "n_components = 2\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "X_train, X_test = X_train.astype('float32')/255, X_test.astype('float32')/255\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "#The architecture we gonna pruning\n",
        "input = Input((32, 32, 3))\n",
        "H = Conv2D(16, (3,3), padding='same')(input)\n",
        "H = Activation('relu')(H)\n",
        "H = Conv2D(16, (3, 3))(H)\n",
        "H = Activation('relu')(H)\n",
        "H = MaxPooling2D(pool_size=(2, 2))(H)\n",
        "print('Ritik')\n",
        "H = Conv2D(32, (3, 3), padding='same')(H)\n",
        "H = Activation('relu')(H)\n",
        "H = Conv2D(32, (3, 3))(H)\n",
        "H = Activation('relu')(H)\n",
        "H = MaxPooling2D(pool_size=(2, 2))(H)\n",
        "\n",
        "H = Flatten()(H)\n",
        "H = Dense(512)(H)\n",
        "H = Activation('relu')(H)\n",
        "H = Dropout(0.5)(H)\n",
        "H = Dense(10)(H)\n",
        "H = Activation('softmax')(H)\n",
        "print('Ritik')\n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
        "cnn_model = keras.models.Model([input], H)\n",
        "cnn_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "cnn_model.fit(X_train, y_train, epochs=epochs, batch_size=128, verbose=0)\n",
        "y_pred = cnn_model.predict(X_test)\n",
        "acc = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
        "print('Ritik')\n",
        "n_params = cnn_model.count_params()\n",
        "n_filters = count_filters(cnn_model)\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ritik\n",
            "Ritik\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ritik\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g7KOtet1lYm",
        "outputId": "aba6c778-416b-4625-c9a0-79090b5e1398"
      },
      "source": [
        "flops, _ = compute_flops(cnn_model)\n",
        "print('Original Network. #Parameters [{}] #Filters [{}] FLOPs [{}] Accuracy [{:.4f}]'.format(n_params, n_filters, flops, acc))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Network. #Parameters [612122] #Filters [96] FLOPs [5705216] Accuracy [0.5680]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etgm1ZW1jhJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8dfd17c-c2ad-4371-fda8-db7158cef2af"
      },
      "source": [
        "layers = layers_to_prune(cnn_model)\n",
        "\n",
        "for i in range(0, iterations):\n",
        "\n",
        "    pruning_method = VIPPruning(n_comp=n_components, model=cnn_model, representation='max', percentage_discard=p)\n",
        "    # pruning_method = VIPPruning(n_comp=n_components, model=cnn_model,\n",
        "    #                             representation=MaxPooling2D(pool_size=(2, 2),\n",
        "    #                                                         name='vip_net'),\n",
        "    #                             percentage_discard=p)\n",
        "\n",
        "    idxs = pruning_method.idxs_to_prune(X_train, y_train, layers)\n",
        "    cnn_model = rebuild_net(cnn_model, idxs)\n",
        "\n",
        "    cnn_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    cnn_model.fit(X_train, y_train, epochs=epochs, batch_size=128, verbose=0)\n",
        "\n",
        "    y_pred = cnn_model.predict(X_test)\n",
        "    acc = accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))\n",
        "\n",
        "    n_params = cnn_model.count_params()\n",
        "    n_filters = count_filters(cnn_model)\n",
        "    flops, _ = compute_flops(cnn_model)\n",
        "    print('Iteration [{}] #Parameters [{}] #Filters [{}] FLOPs [{}] Accuracy [{:.4f}]'.format(i, n_params, n_filters, flops, acc))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration [0] #Parameters [611363] #Filters [93] FLOPs [5212520] Accuracy [0.6283]\n",
            "Iteration [1] #Parameters [610613] #Filters [90] FLOPs [4835177] Accuracy [0.6637]\n",
            "Iteration [2] #Parameters [609395] #Filters [87] FLOPs [4521158] Accuracy [0.6882]\n",
            "Iteration [3] #Parameters [608933] #Filters [84] FLOPs [4098014] Accuracy [0.6984]\n",
            "Iteration [4] #Parameters [608210] #Filters [81] FLOPs [3738896] Accuracy [0.7089]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS-vB1cRLTwL"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}